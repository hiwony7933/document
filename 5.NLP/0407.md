

```python
import pandas as pd 
import numpy as np 
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split # train test 나누기 
from sklearn.metrics import accuracy_score # 평가방법 

t_df = pd.read_pickle('./dataset/t_df.pkl')
print(t_df.head(5))

y_df = t_df.survived
X_df = t_df.drop('survived', axis=1)
X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=0.2, random_state=0)
dt_model = DecisionTreeClassifier(random_state=0)
dt_model.fit(X_train, y_train)

dt_pred = dt_model.predict(X_test)
at_accuracy = accuracy_score(y_test, dt_pred)
print("예측정확도:", at_accuracy)

from sklearn.linear_model import LogisticRegression

lr_model = LogisticRegression(random_state=0)
lr_model.fit(X_train, y_train)
lr_pred = lr_model.predict(X_test)

lr_accuracy = accuracy_score(y_test, lr_pred)
print("예측정확도:", lr_accuracy)

```


```python
# 사용자 함수 

from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score
def get_clf_eval(y_test, pred) : 
    confusion = confusion_matrix(y_test, pred)
    accuracy = accuracy_score(y_test, pred)
    precision = precision_score(y_test, pred)
    recall = recall_score(y_test, pred)
    f1 = f1_score(y_test, pred)
    print('오차행렬')
    print(confusion)
    print('정확도:{0:.4f}, 정밀도 : {1:.4f}, 재현율:{2:.4f}, F1:{3:.4f}'. format(accuracy, precision, recall, f1))
    print()

get_clf_eval(y_test, dt_pred)
get_clf_eval(y_test, lr_pred)
```

* 출력 

```bash
오차행렬
[[135  27]
 [ 36  64]]
정확도:0.7595, 정밀도 : 0.7033, 재현율:0.6400, F1:0.6702

오차행렬
[[143  19]
 [ 35  65]]
정확도:0.7939, 정밀도 : 0.7738, 재현율:0.6500, F1:0.7065
```



### 교차 검증 

* 학습데이터 와 검증데이터 나눠서
* 학습데이터로 학습시키고 개발한 모델의 성능 평가 

* 학습데이터, 검증데이터 간의 비율 문제 -> 문제가 생긴다. 
* 검증데이터가 한쪽방향으로 치울칠수 있음(데이터제한) -> 외곡, 편항되게 검증됨. 
* 있는 데이터로 최대한 해보자 -> 교차 검증 

* KFold : 데이터가 편황될경우 문제 가 있음 - 특이한 경우 사용  
* Cross_val_scores : 주로 많이 쓴다 코드 간결화 - KFole의 일련의 과정을 한꺼번에 수행해주는 API 


각각의 20%를 검증용으로 사용하고 나머지를 학습데이터로 사용 한다. 

K | 1 | 2 | 3 | 4 | 5 |
-- | -- | -- | -- | -- | -- |
1 | // | | | | |
2 | | // | | | |
3 | | | // | | |
4 | | | | // | |
5 | | | | | // |

```python 
# 교차 검증
from sklearn.model_selection import cross_val_score

scores = cross_val_score(dt_model, X_df, y_df, cv=10) # cv=10 번의 교차검증
for iter_count, accuracy in enumerate(scores) : 
    print('교차검증 {0} 정확도 : {1:.4f}'. format(iter_count, accuracy))
print('평균정확도:{0:.4f}'.format(np.mean(scores)))
```

### GridSearchCV 

DT 파라미터 
max_depth : 트리의 최대 깊이 
max_features : 최적의 분할을 위해 고려할 최대 피쳐 개ㅜㅅ 


파생변수 - > 적합한 알고리즘 (매개변수(파라미터)를 어떻게 주느냐에 따라 달라짐(규제))

규제 - 과대적합(차원의저주)을 방지할수 있는 포인트 

학습데이터-> 잘 훈련은 되어있는데 일반화가 안되어 부정적인 효과가 나타남 -> 하이퍼 파라미터 튜닝 

최적의 파라미터를 제시해서 적용해서 튜닝 

```python
# GridSearchCV
# 모델 평가 

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score

parameters = {'max_depth':[2, 3, 5, 10], 'min_samples_split' :[2,3,5], 'min_samples_leaf':[1, 5, 8]}

# refit=최적의 하이퍼파라미터를 적용하겠다. 
# 각각의 파라미터의 경우의수를 적용해서 모델학습을 시켜보고 5번 교체검증을 하고, 평균이 나옴 
grid_model = GridSearchCV(dt_model, param_grid=parameters, scoring='accuracy', cv=5, refit=True) 

grid_model.fit(X_train, y_train)
print(grid_model)

# 교차검증을 기반으로 최적의 하이퍼 파라미터를 찾아줌 
print('GridSearchCV 최적 하이퍼 파라미터:', grid_model.best_params_)
print('GridSearchCV 최고 정확도:{0:.4f}'.format(grid_model.best_score_))

# 최적의 하이퍼 파라미터를 적용해서 
best_dclf = grid_model.best_estimator_  # 최적의 하이퍼파라미터를 적용한것 
print(best_dclf)
dt_pred = best_dclf.predict(X_test)
accuracy = accuracy_score(y_test, dt_pred)
print("dt 예측 정확도", accuracy)

get_clf_eval(y_test, dt_pred)
```

* 결과 

```bash
ridSearchCV(cv=5, error_score=nan,
             estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                              criterion='gini', max_depth=None,
                                              max_features=None,
                                              max_leaf_nodes=None,
                                              min_impurity_decrease=0.0,
                                              min_impurity_split=None,
                                              min_samples_leaf=1,
                                              min_samples_split=2,
                                              min_weight_fraction_leaf=0.0,
                                              presort='deprecated',
                                              random_state=0, splitter='best'),
             iid='deprecated', n_jobs=None,
             param_grid={'max_depth': [2, 3, 5, 10],
                         'min_samples_leaf': [1, 5, 8],
                         'min_samples_split': [2, 3, 5]},
             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,
             scoring='accuracy', verbose=0)
GridSearchCV 최적 하이퍼 파라미터: {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 2}
GridSearchCV 최고 정확도:0.8070
DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=3, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, presort='deprecated',
                       random_state=0, splitter='best')
dt 예측 정확도 0.7938931297709924
오차행렬
[[146  16]
 [ 38  62]]
정확도:0.7939, 정밀도 : 0.7949, 재현율:0.6200, F1:0.6966
```



