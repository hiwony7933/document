
## 문서 군집화




## 문서 유사도 

* similarity  - 방향성이 같은 것은 좀더 유사도가 높다. 

### 문서와 문서 간의 유사도 비교

* 코사인 유사도는 벡터와 벡터 간의 유사도 비교시 벡터의 상호 방향성이 얼마나 유사한지에 기반 즉 두 벡터 사이의 사잇각을 구해서 얼마나 유사한지 수치로 적용
* 희소 행렬 기반에서 문서와 문서 벡터간의 크기에 기반한 유사도 지표는 정확도가 떨어지기가 쉬움
* 또한 문서가 매우 긴 경우 단어의 빈도수가 더 많을 것이므로 빈도수에만 기반해서는 공정한 비교가 어려움 A문서에서 '머신러닝' 단어가 5번 언급되고 B문서에서는 3번 언급되었을 때 A문서가 B문서보다 10배 이상 크다면 오히려 B문서가  더 관련된 문서라고 볼 수 있음

$$
similarity = cos(Θ) =\frac {A*B}{||A|| * ||B||} = \frac{\sum A*B} {\sqrt{\sum {A^2}} * \sqrt{\sum {B^2}}}
$$

* 공식을 사용자 함수로 

```python
import numpy as np 
def cos_similarity(v1, v2) : 
    dot_product = np.dot(v1, v2)
    l2_norm = (np.sqrt(sum(np.square(v1)))*np.sqrt(sum(np.square(v2))))
    similarity = dot_product / l2_norm
    return similarity
```

```python
from sklearn.feature_extraction.text import TfidfVectorizer
doc_list = ['if you take the blue pill, the story ends' ,
            'if you take the red pill, you stay in Wonderland',
            'if you take the red pill, I show you how deep the rabbit hole goes']
tfidf_vect_simple = TfidfVectorizer()
feature_vect_simple = tfidf_vect_simple.fit_transform(doc_list)
print(feature_vect_simple.shape)

# TFidfVectorizer로 transform()한 결과는 Sparse Matrix이므로 Dense Matrix로 변환. 
feature_vect_dense = feature_vect_simple.todense()

#첫번째 문장과 두번째 문장의 feature vector  추출
vect1 = np.array(feature_vect_dense[0]).reshape(-1,)
vect2 = np.array(feature_vect_dense[1]).reshape(-1,)

#첫번째 문장과 두번째 문장의 feature vector로 두개 문장의 Cosine 유사도 추출
similarity_simple = cos_similarity(vect1, vect2 )
print('문장 1, 문장 2 Cosine 유사도: {0:.3f}'.format(similarity_simple))

#출력
(3, 18)
문장 1, 문장 2 Cosine 유사도: 0.402
```

## 토픽 모델링 

* 머신러닝 기반의 토픽 모델링을 적용해 문서 집합에 숨어 있는 주제를 찾아냄
* 사람이 수행하는 토픽 모델링은 더 함축적인 의미로 문장을 요약하는 것에 반해 머신러닝 기반의 토픽 모델링은 숨겨진 주제를 효과적으로 표현할 수 있는 중심 단어를 함축적으로 추출
* LSA(Latent Sementic Analysis) 와 LDA(Latent Dirichlet Allocation) 기법
  * LSA는 단어-문서행렬(Word-Document Matrix), 단어-문맥행렬(window based co-occurrence matrix) 등 입력 데이터에 특이값 분해를 수행해 데이터의 차원수를 줄여 계산 효율성을 키우면서 행간에 숨어있는(latent) 의미를 이끌어내기 위한 방법론
  * LDA는 미리 알고 있는 주제별 단어수 분포를 바탕으로, 주어진 문서에서 발견된 단어수 분포를 분석, 해당 문서가 어떤 주제들을 함께 다루고 있을지를 예측



* argsort() 넘파이 배열의 원소를 오름차순으로 정렬하는 메소드

```python
import numpy as np 
d1 = np.arange(10, 25)
print(d1) 
d2 = d1.argsort()
print(d2)
topic_word_indexes = d1.argsort() [::-1]
print(topic_word_indexes)
top_indexes = topic_word_indexes[:10]
top_indexes

#출력
[10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]
[14 13 12 11 10  9  8  7  6  5  4  3  2  1  0]
array([14, 13, 12, 11, 10,  9,  8,  7,  6,  5])
```


fetch_20newsgroups 

TfidfVectorizer 방식으로 벡터 처리하고 lr 알고리즘으로 preciasdlfadsfljaksdfjkl


로이터 뉴스를 46개의 상호 배타적인 토픽으로 분류하는 신경망 모델 개발